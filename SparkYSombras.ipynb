{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.util.Random\n",
    "import akka.stream.scaladsl.{Flow, Source}\n",
    "import akka.actor.ActorSystem\n",
    "import akka.stream.ActorMaterializer\n",
    "import scala.concurrent.Future\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "implicit val as = ActorSystem()\n",
    "implicit val am = ActorMaterializer()\n",
    "\n",
    "val random = new Random()\n",
    "\n",
    "spark.read.json(\"spanish-la-liga/data/*.json\")\n",
    "    .select(\n",
    "        func.explode(\n",
    "            func.array_repeat(\n",
    "                func.struct(\n",
    "                    $\"HomeTeam\".alias(\"home\"), \n",
    "                    $\"AwayTeam\".alias(\"away\"), \n",
    "                    $\"FTHG\".alias(\"homeGoals\"), \n",
    "                    $\"FTAG\".alias(\"awayGoals\"), \n",
    "                    $\"HTHG\".alias(\"homeGoalsHT\"), \n",
    "                    $\"HTAG\".alias(\"awayGoalsHT\"),\n",
    "                    func.current_date().alias(\"date\"),\n",
    "                    func.lit(random.alphanumeric.take(1000).toString).alias(\"description\")\n",
    "                ),\n",
    "                1000\n",
    "            )\n",
    "        ).alias(\"value\")\n",
    "    )\n",
    "    .selectExpr(\"value.*\")\n",
    "    .write.mode(SaveMode.Overwrite).parquet(\"data/la-liga.parquet\")\n",
    "\n",
    "Seq(\n",
    "    \"Andalucía\" -> Seq(\"Betis\", \"Malaga\", \"Xerez\", \"Almeria\", \"Granada\"),\n",
    "    \"C. Valenciana\" -> Seq(\"Villarreal\", \"Elche\"),\n",
    "    \"Catalunya\" -> Seq(\"Espanyol\", \"Barcelona\"),\n",
    "    \"Madrid\" -> Seq(\"Ath Madrid\", \"Real Madrid\", \"Vallecano\", \"Leganes\"),\n",
    "    \"Euskadi\" -> Seq(\"Ath Bilbao\", \"Eibar\"),\n",
    "    \"Galiza\" -> Seq(\"Celta\", \"La Coruna\"),\n",
    "    \"Cantabria\" -> Seq(\"Santander\"),\n",
    "    \"Aragón\" -> Seq(\"Zaragoza\"),\n",
    "    \"Canarias\" -> Seq(\"Tenerife\")\n",
    ").flatMap{\n",
    "    case (region, teams) => teams.map((_, region))\n",
    "}.toDF.select(\n",
    "    $\"_1\".alias(\"team\"),\n",
    "    $\"_2\".alias(\"region\")\n",
    ")\n",
    ".write.mode(SaveMode.Overwrite).parquet(\"data/dictionary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK UI http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ligaDF = spark.read.parquet(\"data/la-liga.parquet\") // Eager execution (schema read)\n",
    "ligaDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligaDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val homeGoalsPerTeam =\n",
    "    ligaDF\n",
    "        .groupBy($\"home\".alias(\"team\"))\n",
    "        .agg(func.sum($\"homeGoals\").alias(\"goals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeGoalsPerTeam.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeGoalsPerTeam.write.mode(SaveMode.Overwrite).parquet(\"aggregation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"aggregation.parquet\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations\n",
    "\n",
    "## Pushdown filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligaDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val goalsPerTeam = \n",
    "    ligaDF\n",
    "        .where($\"away\" =!= \"Betis\")\n",
    "        .select(\n",
    "        func.explode(\n",
    "            func.array(\n",
    "                func.struct(\n",
    "                    $\"home\".alias(\"team\"),\n",
    "                    $\"homeGoals\".alias(\"goals\"),\n",
    "                ),\n",
    "                func.struct(\n",
    "                    $\"away\".alias(\"team\"),\n",
    "                    $\"awayGoals\".alias(\"goals\"),\n",
    "                )\n",
    "            )\n",
    "        ).alias(\"team\")\n",
    "    ).selectExpr(\"team.*\")\n",
    "    \n",
    "    .groupBy($\"team\")\n",
    "    .agg(func.sum($\"goals\").alias(\"goals\"))\n",
    "\n",
    "\n",
    "goalsPerTeam.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "    goalsPerTeam\n",
    "        .write.mode(SaveMode.Overwrite).parquet(\"data/goals-per-team-df.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this) // Necessary to declare case classes\n",
    "import java.util.Date\n",
    "\n",
    "case class Match(\n",
    "    home: String,\n",
    "    away: String,\n",
    "    homeGoals: Option[Long],\n",
    "    awayGoals: Option[Long],\n",
    "    homeGoalsHT: Option[Long],\n",
    "    awayGoalsHT: Option[Long],\n",
    "    description: String\n",
    ")\n",
    "\n",
    "ligaDF\n",
    "    .as[Match]\n",
    "    .flatMap{\n",
    "        m => \n",
    "         Seq((m.home, m.homeGoals), (m.away, m.awayGoals) )\n",
    "    }.groupByKey(_._1)\n",
    "    .mapValues(_._2.getOrElse(0L))\n",
    "    .reduceGroups(_ + _)\n",
    "    .toDF(\"team\", \"goals\")\n",
    "    .write.mode(SaveMode.Overwrite).parquet(\"data/goals-per-team-df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/goals-per-team-df.parquet\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PushDown from inner stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/dictionary.parquet\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "val teamsMetadata =  spark.read.parquet(\"data/dictionary.parquet\")\n",
    "\n",
    "run(\n",
    "    ligaDF\n",
    "        .join(teamsMetadata, $\"home\" === $\"team\")\n",
    "        .where($\"home\" === \"Betis\")\n",
    "        .write.mode(SaveMode.Overwrite).parquet(\"data/teams-no-betis-df.parquet\")\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/teams-no-betis-df.parquet\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this) // Necessary to declare case classes\n",
    "\n",
    "case class Match(\n",
    "    home: String,\n",
    "    away: String,\n",
    "    homeGoals: Option[Long],\n",
    "    awayGoals: Option[Long],\n",
    "    homeGoalsHT: Option[Long],\n",
    "    awayGoalsHT: Option[Long],\n",
    "    description: String\n",
    ")\n",
    "\n",
    "case class TeamDictionary(\n",
    "    team: String,\n",
    "    region: String\n",
    ")\n",
    "\n",
    "val teamsMetadata =  spark.read.parquet(\"data/dictionary.parquet\")\n",
    "\n",
    "run(\n",
    "    ligaDF\n",
    "        .as[Match]\n",
    "        .joinWith(teamsMetadata.as[TeamDictionary], $\"home\" === $\"team\")\n",
    "        .filter(_._1.home != \"Betis\")\n",
    "        .selectExpr(\"_1.*\", \"_2.*\")\n",
    "        .write.mode(SaveMode.Overwrite).parquet(\"data/teams-no-betis-ds.parquet\")\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/teams-no-betis-ds.parquet\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def someHardCalculation(v: Long): Long = {\n",
    "    Thread.sleep(20) // simulates a computationally expensive operation\n",
    "    v * 10\n",
    "}\n",
    "\n",
    "// This transformation takes some time\n",
    "val computedData = spark.range(0, 1000).map(someHardCalculation(_))\n",
    "\n",
    "val cachedDS = computedData.cache() // equivalent to computedData.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedDS.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(cachedDS.reduce(_ + _))\n",
    "\n",
    "run(cachedDS.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not so good\n",
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computedData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedDS.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(cachedDS.reduce(_ + _))\n",
    "\n",
    "run(cachedDS.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"data2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder generation unchecked at compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this) // Necessary to declare case classes\n",
    "import java.util.Date\n",
    "\n",
    "case class Match(\n",
    "    home: String,\n",
    "    away: String,\n",
    "    homeGoals: Option[Long],\n",
    "    awayGoals: Option[Long],\n",
    "    homeGoalsHT: Option[Long],\n",
    "    awayGoalsHT: Option[Long],\n",
    "    date: Date\n",
    ")\n",
    "\n",
    "def readMatches(): Dataset[Match] = {\n",
    "    ligaDF.as[Match]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readMatches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more optimizations could be added to Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/la-liga.parquet\")\n",
    "    .select($\"away\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this) // Necessary to declare case classes\n",
    "\n",
    "case class Match(\n",
    "    home: String,\n",
    "    away: String,\n",
    "    homeGoals: Option[Long],\n",
    "    awayGoals: Option[Long],\n",
    "    homeGoalsHT: Option[Long],\n",
    "    awayGoalsHT: Option[Long]\n",
    ")\n",
    "\n",
    "ligaDF\n",
    "    .as[Match]\n",
    "    .map(_.away)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-grained parallelism\n",
    "\n",
    "When a function's complexity depends on the row value it's not so easy to parallelize on Spark\n",
    "\n",
    "### Spark naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def someHardComputation(v: Long): Long = {\n",
    "    Thread.sleep(10)\n",
    "    v * 10L\n",
    "}\n",
    "\n",
    "run(\n",
    "    spark\n",
    "        .range(0L, 100L)\n",
    "        .map(\n",
    "            i => if(i == 0) 10000L else i.toLong\n",
    "        )\n",
    "        .repartition(4).map(\n",
    "    i => 0L.to(i.toLong).map(someHardComputation(_)).sum \n",
    ").reduce(_ + _)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"composition.parquet\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akka Stream naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val serie = 0L.to(100L).map(\n",
    "            i => if(i == 0) 10000L else i.toLong\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runAsync(\n",
    "    Source(serie)\n",
    "         .map{\n",
    "            v =>\n",
    "                v -> 0L.to(v).map(someHardCalculation).sum\n",
    "        }\n",
    "        .runFold(List.empty[(Long, Long)])((l, t) => t :: l)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akka stream optimized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def someHardCalculation(v: Long) = Future {\n",
    "    Thread.sleep(20) // simulates a computationally expensive operation\n",
    "    v * 10\n",
    "}\n",
    "\n",
    "// Empezar por el programa malo\n",
    "\n",
    "def calculate(v: Long) = \n",
    "    Source.fromIterator(() => 0L.to(v).iterator)\n",
    "        .mapAsync(4)(someHardCalculation)\n",
    "        .reduce(_ + _)\n",
    "        .map(res => v -> res)\n",
    "\n",
    "runAsync(\n",
    "    Source(serie)\n",
    "        .flatMapConcat(calculate)\n",
    "        .runFold(List.empty[(Long, Long)])((l, t) => t :: l)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark optimized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def someHardCalculation(v: Long): Long = {\n",
    "    Thread.sleep(20) // simulates a computationally expensive operation\n",
    "    v * 10\n",
    "}\n",
    "\n",
    "run(\n",
    "    spark\n",
    "        .range(0L, 100L)\n",
    "        .map(\n",
    "            i => if(i == 0) 10000L else i.toLong\n",
    "        )\n",
    "        .flatMap(\n",
    "            i =>\n",
    "                0L.to(i).map(j => (i, j))\n",
    "        )\n",
    "        .map{\n",
    "            case (i, j) => i -> someHardComputation(j)\n",
    "        }\n",
    "        .groupBy($\"_1\".alias(\"id\"))\n",
    "        .agg(func.sum($\"_2\").alias(\"value\"))\n",
    "        .select($\"value\")\n",
    "        .as[Long]\n",
    "        .reduce(_ + _)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"composition.parquet\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
